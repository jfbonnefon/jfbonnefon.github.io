





@article{doi:10.1146/annurev-psych-030123-113559,
author = {Bonnefon, Jean-Fran\c{c}ois and Rahwan, Iyad and Shariff, Azim},
title = {The Moral Psychology of Artificial Intelligence},
journal = {Annual Review of Psychology},
volume = {75},
number = {1},
pages = {653-675},
year = {2024},
doi = {10.1146/annurev-psych-030123-113559},
    note ={PMID: 37722750},

URL = { 
    
        https://doi.org/10.1146/annurev-psych-030123-113559
    
    

},
eprint = { 
    
        https://doi.org/10.1146/annurev-psych-030123-113559
    
    

}
,
    abstract = { Moral psychology was shaped around three categories of agents and patients: humans, other animals, and supernatural beings. Rapid progress in artificial intelligence has introduced a fourth category for our moral psychology to deal with: intelligent machines. Machines can perform as moral agents, making decisions that affect the outcomes of human patients or solving moral dilemmas without human supervision. Machines can be perceived as moral patients, whose outcomes can be affected by human decisions, with important consequences for humanâ€“machine cooperation. Machines can be moral proxies that human agents and patients send as their delegates to moral interactions or use as a disguise in these interactions. Here we review the experimental literature on machines as moral agents, moral patients, and moral proxies, with a focus on recent findings and the open questions that they suggest. }
}


